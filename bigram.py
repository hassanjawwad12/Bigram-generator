# -*- coding: utf-8 -*-
"""Bigram.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HkiAls8C66P019nxkjSlihu2bSV2Kx7e
"""

with open('wizard.txt', 'r', encoding='utf-8') as f:
    text = f.read()

#this prints the length of the text
print(len(text))

print(text[:200])

#All the charcters that we have
chars = sorted(set(text))
print(chars)

vocab_size = len(chars)

"""Now we will make an ecoder and a decoder.
This is a character level tokenizer which takes each charcater and converts it to an integer.
We have small vocab but large amount of tokens to be converted.

If we work with word level tokenizer then it would be trillions of tokens. Large vocab but small tokens.

Sub-word tokenizer is hybrid of both


"""

string_to_int = { ch:i for i,ch in enumerate(chars) }
int_to_string = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l])

print(encode('Hassan'))

print(decode([32, 54, 72, 72, 54, 67]))

import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

"""Some cool stuff you can do with pytorch so you know
what it is actually used for

"""

import torch.nn as nn
from torch.nn import functional as F
import numpy as np
import time

# Commented out IPython magic to ensure Python compatibility.
# %%time
# start_time = time.time()
# # matrix operations here
# zeros = torch.zeros(1, 1)
# end_time = time.time()
# 
# elapsed_time = end_time - start_time
# print(f"{elapsed_time:.8f}")
#

randint=torch.randint(-100,100,(2,))
print(randint)

tensor=torch.tensor([[1,2],[3,4],[5,6]])
print(tensor)

input=torch.empty(2,2)
print(input)

arange=torch.arange(5)
print(arange)

linspace=torch.linspace(3,10,steps=5)
print(linspace)

eye=torch.eye(5)
print(eye)

"""CPU PERFORMS BETTER THAN GPU ON SIMPLE SHAPES

"""

torch_rand1 = torch.rand(100, 100, 100, 100).to(device)
torch_rand2 = torch.rand(100, 100, 100, 100).to(device)
np_rand1 = torch.rand(100, 100, 100, 100)
np_rand2 = torch.rand(100, 100, 100, 100)

start_time = time.time()

rand = (torch_rand1 @ torch_rand2)

end_time = time.time()

elapsed_time = end_time - start_time
print(f"{elapsed_time:.8f}")


start_time = time.time()

rand = np.multiply(np_rand1, np_rand2)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"{elapsed_time:.8f}")

# Define a probability tensor
probabilities = torch.tensor([0.1, 0.9])
# 10% chance of getting a zero, 90% chance for getting a 1. each probability points to the index of the probability in the tensor
# Draw 5 samples from the multinomial distribution
samples = torch.multinomial(probabilities, num_samples=10, replacement=True)
print(samples)

#concatenate 2 sensors into one (used in text generation)
tensor = torch.tensor([1, 2, 3, 4])
out = torch.cat((tensor, torch.tensor([5])), dim=0)
out

#lower triangle
out = torch.tril(torch.ones(5, 5))
out

#upper triangle
out = torch.triu(torch.ones(5, 5))
out

"""Moving from negative infinity to 0, then 0 to 1

"""

out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))
out

torch.exp(out)

input = torch.zeros(2, 3, 4)
out1 = input.transpose(0, 1)
out2 = input.transpose(-2,-1)
print(out1.shape)
print(out2.shape)
# torch.permute works the same but you provide the new order of dimensions instead of the dimensions you'd like to swap.

tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])
tensor3 = torch.tensor([7, 8, 9])

# Stack the tensors increase the dimension
stacked_tensor = torch.stack([tensor1, tensor2, tensor3])
stacked_tensor

import torch.nn as nn
sample = torch.tensor([10.,10.,10.])
linear = nn.Linear(3, 3, bias=False)
print(linear(sample))
#learns weight and bias to become better and better the nn module

"""Softmax converts vector to probabilities

"""

import torch.nn.functional as F

# Create a tensor
tensor1 = torch.tensor([1.0, 2.0, 3.0])

# Apply softmax using torch.nn.functional.softmax()
softmax_output = F.softmax(tensor1, dim=0)

print(softmax_output)

"""N dot embedding is a technique for creating dense vector representations of graphs by multiplying node features and adjacency matrices, capturing relationships between nodes in a low-dimensional space."""

# Initialize an embedding layer
vocab_size = 80
embedding_dim = 6
embedding = nn.Embedding(vocab_size, embedding_dim)

# Create some input indices
input_indices = torch.LongTensor([1, 5, 3, 2])

# Apply the embedding layer
embedded_output = embedding(input_indices)

# The output will be a tensor of shape (4, 100), where 4 is the number of inputs
# and 100 is the dimensionality of the embedding vectors
print(embedded_output.shape)
print(embedded_output)

a = torch.tensor([[1,2],[3,4],[5,6]])
b = torch.tensor([[7,8,9],[10,11,12]])
print(torch.matmul(a, b))

print(a @ b)

int_64 = torch.randint(1, (3, 2)).float()
#type int64
float_32 = torch.rand(2,3)
#type float32
print(int_64.dtype, float_32.dtype)
result = torch.matmul(int_64, float_32)
print(result)

a = torch.rand(2, 3, 5)
print(a.shape)
x, y, z = a.shape
a = a.view(x,y,z)
print(x, y, z)
print(a.shape)

input = torch.rand((4, 8, 10))
B, T, C = input.shape
output = input.view(B*T, C)
print(output)

import matplotlib.pyplot as plt

# Create a heatmap to visualize the transformation (channels vs unfolded elements)
plt.imshow(output.detach().cpu(), aspect='auto')
plt.colorbar(label='Value')
plt.xlabel('Unfolded Elements (B*T)')
plt.ylabel('Channels (C)')
plt.title('Input Tensor Reshaped for Visualization')
plt.show()

x = torch.tensor([10], dtype=torch.float32)
y = F.tanh(x)
print(y)

"""
**Back to our working code**

Define the block,batch size etc

"""

block_size = 8
batch_size = 4
max_iters = 1000
# eval_interval = 2500
learning_rate = 3e-4
eval_iters = 250

#encode the whole txt file data we have
data = torch.tensor(encode(text), dtype=torch.long)

print(data[:50])

"""Split the data to make the generations unique.
Pick a random block size(encoded characters) to make predictions.
"""

n = int(0.8*len(data))
train_data = data[:n]
val_data = data[n:]

block_size=8
x=train_data[:block_size]
y=val_data[1:block_size+1]

for t in range (block_size):
  context = x[:t+1]
  target = y[t]
  print(f"when input is {context} the target is {target}")

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

x, y = get_batch('train')
print('inputs:')
print(x)
print('targets:')
print(y)

print(x.shape)

#we dont need any computation or gradiemt optimization here because we are only reorting loss
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

"""The class takes the nn dot module.

self.token_embedding_table creates a lookup table called token_embedding_table. It's a core component of the model that maps each word in your vocabulary to a numerical vector (embedding). This vector captures some meaning or representation of the word.



"""

class BigramLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, index, targets=None):
        logits = self.token_embedding_table(index)


        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            #looks up the embeddings for each word index in index using the token_embedding_table.
            #logits are probability distrinution of what we want to predict
            #view help us reshape how logits look like
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, index, max_new_tokens):
        # index is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # get the predictions
            logits, loss = self.forward(index)
            # focus only on the last time step
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            index = torch.cat((index, index_next), dim=1) # (B, T+1)
        return index

model = BigramLanguageModel(vocab_size)
m = model.to(device)

context = torch.zeros((1,1), dtype=torch.long, device=device)
generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())
print(generated_chars)

# create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):
    if iter % eval_iters == 0:
        losses = estimate_loss()
        print(f"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}")

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model.forward(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
print(loss.item())

context = torch.zeros((1,1), dtype=torch.long, device=device)
generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())
print(generated_chars)